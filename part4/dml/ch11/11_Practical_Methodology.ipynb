{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11. Practical Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 손고리즘ML : 파트 4 - DML [1]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Contents\n",
    "* 11.1 Performance Metrics\n",
    "* 11.2 Default Baseline Models\n",
    "* 11.3 Determining Whether to Gather More Data\n",
    "* 11.4 Selecting Hyperparameters\n",
    "    - 11.4.1 Manual Hyperparameter Tuning\n",
    "    - 11.4.2 Automatic Hyperparameter Optimization Algorithms\n",
    "    - 11.4.3 Grid Search\n",
    "    - 11.4.4 Random Search\n",
    "    - 11.4.5 Model-Based Hyperparameter Optimization\n",
    "* 11.5 Debugging Strategies\n",
    "* 11.6 Example: Multi-Digit Number Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successfully applying deep learning techniques \n",
    "* requires more than just a good knowledge of \n",
    "    - <font color=\"blue\">what algorithms exist</font> and \n",
    "    - the <font color=\"blue\">principles that explain how they work</font>. \n",
    "* A good machine learning practitioner also needs to know \n",
    "    - <font color=\"red\">how to choose an algorithm for a particular application</font> and \n",
    "    - <font color=\"red\">how to monitor and respond to feedback</font>\n",
    "        - obtained from experiments in order to improve a machine learning system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During day to day development of machine learning systems, practitioners need to decide\n",
    "* whether to gather more data, \n",
    "* increase or decrease model capacity, \n",
    "* add or remove regularizing features, \n",
    "* improve the optimization of a model, \n",
    "* improve approximate inference in a model, or \n",
    "* debug the software implementation of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.svms.org/srm/Sewell2006.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://nbviewer.jupyter.org/github/songorithm/ML/blob/master/part2/study01/dml05/figures/fig5.6.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://yosinski.com/mlss12/media/slides/MLSS-2012-Fukumizu-Kernel-Methods-for-Statistical-Learning_050.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://nbviewer.jupyter.org/github/songorithm/ML/blob/master/part2/study04/dml07/figures/cap7.21.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://nbviewer.jupyter.org/github/songorithm/ML/blob/master/part2/study04/dml07/figures/cap7.48.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of this book is about diﬀerent machine learning models, training algorithms, and objective functions. \n",
    "This may give the impression that the mostimportant ingredient to being a machine learning expert is knowing a wide varietyof machine learning techniques and being good at diﬀerent kinds of math.\n",
    "<font color=\"red\">In practice, one can usually do much better with a correct application of a commonplace algorithm than by sloppily applying an obscure algorithm.</font> Correct application ofan algorithm depends on mastering some fairly simple methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend the following practical design process:\n",
    "* <font color=\"red\">Determine your goals</font>\n",
    "     - what error metric to use, and \n",
    "     - your target value for this error metric. \n",
    "     - These goals and error metrics should be driven by the problem that the application is intended to solve.\n",
    "* <font color=\"red\">Establish a working end-to-end pipeline</font> as soon as possible, \n",
    "    - including the estimation of the appropriate performance metrics.\n",
    "* Instrument the system well to <font color=\"red\">determine bottlenecks in performance</font>.\n",
    "    - Diagnose which components are performing worse than expected and\n",
    "    - whether itis due to \n",
    "        - overﬁtting, \n",
    "        - underﬁtting, or \n",
    "        - a defect \n",
    "            - in the data or \n",
    "            - software.\n",
    "* <font color=\"red\">Repeatedly make incremental changes</font> such as \n",
    "    - gathering new data, \n",
    "    - adjusting hyperparameters, or \n",
    "    - changing algorithms, based on speciﬁc ﬁndings from your instrumentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a running example, we will use <font color=\"red\">Street View address number transcription system</font> (Goodfellow et al., 2014d). \n",
    "* The purpose of this application is to add buildings to Google Maps.\n",
    "* Street View cars photograph the buildings and record the GPS coordinates associated with each photograph. \n",
    "* A convolutional network recognizes the address number in each photograph, allowing the Google Mapsdatabase to add that address in the correct location. \n",
    "* The story of how this commercial application was developed gives an example of how to follow the design methodology we advocate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.1 Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what level of performance you desire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining your goals, in terms of which <font color=\"blue\">error metric</font> to use, is a necessary ﬁrst step because your error metric will guide all of your future actions. <font color=\"red\">You should also have an idea of what level of performance you desire</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that for most applications, it is <font color=\"red\">impossible to achieve absolute zero error</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">The amount of training data can be limited</font> for a variety of reasons.\n",
    "* Data collection can require time,money, or human suﬀering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reasonable level of performance to expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can one determine a <font color=\"red\">reasonable level of performance to expect</font>?\n",
    "* Typically,in the academic setting, \n",
    "    - we have some estimate of the error rate that is attainable based on <font color=\"blue\">previously published benchmark results</font>. \n",
    "* In the real-word setting, \n",
    "    - we have some idea of the error rate that is <font color=\"blue\">necessary for an application</font> to be safe, cost-eﬀective, or appealing to consumers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### common performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important consideration besides the target value of the performance metric is the <font color=\"red\">choice of which metric to use</font>.\n",
    "* Several diﬀerent performance metrics may be used to measure the eﬀectiveness of a complete application that includes machine learning components. \n",
    "* These performance metrics are usually diﬀerent from the cost function used to train the model.\n",
    "* As described in Sec. 5.1.2, it is <font color=\"blue\">common to measure</font> \n",
    "    - the accuracy, or equivalently, \n",
    "        <img src=\"http://www.welaptega.com/wp-content/uploads/2014/09/testing-accuracy.jpg\" width=300 />\n",
    "    - the error rate, of a system.\n",
    "        - 1 - accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### more advanced metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 참고\n",
    "* [2] Performance measures in Azure ML: Accuracy, Precision, Recall and F1 Score. - https://blogs.msdn.microsoft.com/andreasderuiter/2015/02/09/performance-measures-in-azure-ml-accuracy-precision-recall-and-f1-score/\n",
    "* [3] Using ROC plots and the AUC measure in Azure ML - https://blogs.msdn.microsoft.com/andreasderuiter/2015/02/09/using-roc-plots-and-the-auc-measure-in-azure-ml/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, many applications require <font color=\"red\">more advanced metrics</font>.\n",
    "* e-mail spam detection\n",
    "    - Rather thanmeasuring the error rate of a spam classiﬁer, we may wish to measure some formof total cost, where the cost of blocking legitimate messages is higher than the costof allowing spam messages.\n",
    "* a binary classiﬁer that is intended to detect somerare event.\n",
    "    - example : medical test for a rare disease\n",
    "    - precision and recall\n",
    "        <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/2000px-Precisionrecall.svg.png\" width=600 />\n",
    "        - PR curve\n",
    "            <img src=\"http://blogs.msdn.com/cfs-filesystemfile.ashx/__key/communityserver-blogs-components-weblogfiles/00-00-01-57-11-metablogapi/1588.image_5F00_thumb_5F00_1E8173C0.png\" width=600 />\n",
    "        - F-score : The F1 Score is the weighted average of Precision and Recall\n",
    "            <img src=\"figures/cap11.1.png\" width=600 />\n",
    "        - Another option is to report the total area lying beneath the PR curve\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### decision criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some applications, it is possible for the machine learning system to refuse to make a decision. This is useful when the machine learning algorithm can estimate <font color=\"red\">how conﬁdent it should be about a decision</font>, especially if a wrong decision can be harmful and if a human operator is able to occasionally take over.\n",
    "* coverage\n",
    "    - Coverage is the <font color=\"red\">fraction of examples for which the machine learning system is able to produce a response</font>\n",
    "    - One can always obtain 100% accuracyby refusing to process any example, but this reduces the coverage to 0%. \n",
    "    - For theStreet View task, the goal for the project was to reach human-level transcription accuracy while maintaining 95% coverage.\n",
    "    - Human-level performance on this taskis 98% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### other metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many other metrics are possible. \n",
    "* We can for example, measure <font color=\"blue\">click-through rates</font>, \n",
    "    - collect <font color=\"blue\">user satisfaction</font> surveys, and so on. \n",
    "        <img src=\"http://www.mailigen.com/blog/wp-content/uploads/2013/08/350x300px.png\" width=600 />\n",
    "* Many specialized application areas have <font color=\"red\">application-speciﬁc criteria</font> as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without clearly deﬁned goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is important is to determine which performance metric to improve ahead of time, then <font color=\"blue\">concentrate on improving this metric</font>. <font color=\"red\">Without clearly deﬁned goals,it can be diﬃcult to tell whether changes to a machine learning system make progress or not</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.2 Default Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After choosing performance metrics and goals, the next step in any practical application is to <font color=\"red\">establish a reasonable end-to-end system as soon as possible</font>. In this section, we provide recommendations for which algorithms to use as the ﬁrst baseline approach in various situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### without using deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Depending on the complexity of your problem, <font color=\"red\">you may even want to begin without using deep learning</font>.\n",
    "* If your problem has a chance of being solved by just choosing a few linear weights correctly, you may want to <font color=\"red\">begin with a simple statistical model like logistic regression</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If you know that your problem falls into an “AI-complete” category like \n",
    "    - object recognition, \n",
    "    - speech recognition, \n",
    "    - machine translation, and so on, \n",
    "* then you are likely to do well by <font color=\"red\">beginning with an appropriate deep learning model</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First, choose the <font color=\"blue\">general category</font> of model <font color=\"red\">based on the structure of your data</font>.\n",
    "    - If you want to perform <font color=\"blue\">supervised learning with ﬁxed-size vectors as input</font> <font color=\"red\"> -> a feedforward network with fully connected layers</font>\n",
    "    <img src=\"http://cs231n.github.io/assets/nn1/neural_net2.jpeg\" width=300 />\n",
    "    - If the <font color=\"blue\">input has known topological structure</font> (for example, if the input is an image) <font color=\"red\"> -> CNN</font>\n",
    "        <img src=\"http://deeplearning.net/tutorial/_images/mylenet.png\" width=600 />\n",
    "        - In these cases, you should begin by using some kind of <font color=\"red\">piecewise linear unit</font> \n",
    "            ##### 참고\n",
    "            * [6] L1 : Deep Neural Networks (Udacity) - https://drive.google.com/file/d/0B3vuuoFuJsKWdFFkMS10N1BpLTg/view\n",
    "\n",
    "            - ReLUs or \n",
    "            <img src=\"http://nn.readthedocs.org/en/rtd/image/relu.png\" width=300 />\n",
    "            - their generalizations like \n",
    "                - Leaky ReLUs, \n",
    "                <img src=\"http://lamda.nju.edu.cn/weixs/project/CNNTricks/imgs/relufamily.png\" width=600 />\n",
    "                - PreLus and \n",
    "                    ##### 참고 \n",
    "                    * [4] Benchmarking ReLU and PReLU using MNIST and Theano - http://gforge.se/2015/06/benchmarking-relu-and-prelu/\n",
    "                    <img src=\"http://gforge.se/wp-content/uploads/2015/05/PReLU.jpg\" width=400 />\n",
    "                - maxout.\n",
    "                    ##### 참고\n",
    "                    * [5] Maxing out the digits - http://fastml.com/maxing-out-the-digits/\n",
    "                    <img src=\"http://fastml.com/images/pylearn2/digits/maxout.png\" width=400 />\n",
    "    - If your <font color=\"blue\">input or output is a sequence</font> <font color=\"red\"> -> gated recurrent net (LSTM or GRU)</font>\n",
    "        <img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png\" width=600 />\n",
    "        <img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\" width=600 />\n",
    "        <img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### optimization algorithm\n",
    "* A reasonable choice of optimization algorithm is \n",
    "    - SGD \n",
    "        - <font color=\"blue\">with momentum</font> \n",
    "            - <font color=\"blue\">with a decaying learning rate</font> \n",
    "            (popular decay schemes that perform better or worse on diﬀerent problems include decaying linearly until reaching a ﬁxed minimum learning rate, decaying exponentially, or decreasing the learning rate by a factor of 2-10 each time validation error plateaus)\n",
    "            #### 참고\n",
    "                - [7] Deeplearning4j Updaters Explained - http://deeplearning4j.org/updater\n",
    "    <img src=\"http://deeplearning4j.org/img/updater_1.png\" width=300 />\n",
    "    <img src=\"http://deeplearning4j.org/img/updater_2.png\" width=300 />\n",
    "    <img src=\"http://i.ytimg.com/vi/s6jC7Wc9iMI/0.jpg\" width=300 />\n",
    "* Another very reasonable alternative is \n",
    "    - Adam\n",
    "    ##### 참고\n",
    "        - [8] An overview of gradient descent optimization algorithms - http://sebastianruder.com/optimizing-gradient-descent/index.html#adam\n",
    "* Batch normalization     \n",
    "    - can have a dramatic eﬀect on optimization performance,\n",
    "    - especially for \n",
    "        - convolutional networks and \n",
    "        - networks with sigmoidal nonlinearities.\n",
    "    - While it is reasonable to omit batch normalization from the very ﬁrst baseline, it should be introduced quickly if optimization appears to be problematic.\n",
    "    \n",
    "    ##### 참고\n",
    "    * [9] Directions in Convolutional Neural Networks at Google - http://vision.stanford.edu/teaching/cs231n/slides/jon_talk.pdf\n",
    "    * [10] Batch Normalization (ICML 2015) - http://sanghyukchun.github.io/88/\n",
    "    <img src=\"http://sanghyukchun.github.io/images/post/88-1.jpg\" width=300 />\n",
    "    <img src=\"http://sanghyukchun.github.io/images/post/88-2.png\" width=400/>\n",
    "    <img src=\"http://sanghyukchun.github.io/images/post/88-5.png\" width=400 />\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### regularization\n",
    "* <font color=\"red\">Unless your training set contains tens of millions</font> of examples or more, you should include <font color=\"red\">some mild forms of regularization from the start</font>.\n",
    "    - Early stopping\n",
    "        <img src=\"http://deeplearning4j.org/img/earlystopping.png\" width=300 />\n",
    "    - Dropout\n",
    "        <img src=\"http://engineering.flipboard.com/assets/convnets/dropout.png\" width=300 />\n",
    "    - Batch normalization\n",
    "* <font color=\"red\">If your task is similar to another task that has been studied</font> extensively, you will probably do well by ﬁrst <font color=\"red\">copying the model and algorithm that is already known to perform best on the previously studied task</font>.\n",
    "    - For example, it is common to use the featuresfrom a convolutional network trained on ImageNet to solve other computer visiontasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### unsupervised learning\n",
    "* A common question is whether to begin by using unsupervised learning, de-scribed further in Part III.\n",
    "* This is somewhat domain speciﬁc.\n",
    "    - NLP\n",
    "        - Some domains, suchas natural language processing, are known to beneﬁt tremendously from unsuper-vised learning techniques such as learning unsupervised word embeddings.\n",
    "    - Computer vision\n",
    "        - In otherdomains, such as computer vision, current unsupervised learning techniques donot bring a beneﬁt, except in the semi-supervised setting, when the number oflabeled examples is very small\n",
    "* If your application is in a context where unsupervised learning is known to be important,then include it in your ﬁrst end-to-end baseline.\n",
    "*  Otherwise, only use unsupervised learning in your ﬁrst attempt if the task you want to solve is unsupervised. \n",
    "    - You can always try adding unsupervised learning later if you observe that your initialbaseline overﬁts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.3 Determining Whether to Gather More Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">After the ﬁrst end-to-end system is established</font>, it is time to measure the perfor-mance of the algorithm and determine how to improve it. Many machine learning novices are tempted to make improvements by trying out many diﬀerent algorithms. However, <font color=\"red\">it is often much better to gather more data</font> than to improve the learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">How does one decide whether to gather more data?</font>\n",
    "* First, determine <font color=\"green\">whether the performance on the training set is acceptable</font>.\n",
    "    -  <font color=\"red\">If performance on the training set is poor</font>, \n",
    "        - the learning algorithm is not using the training data that is already available, so there is no reason to gather more data.\n",
    "        - Instead, \n",
    "            - <font color=\"red\">try increasing the size of the model</font> \n",
    "                - by adding more layers or adding more hidden units to each layer.\n",
    "            - Also, <font color=\"red\">try improving the learning algorithm</font>, \n",
    "                - for example by tuning the learning rate hyperparameter. \n",
    "        - <font color=\"red\">If large models and carefully tuned optimization algorithms do not work well</font>, then the problem might be the <font color=\"red\">quality of the training data</font>.\n",
    "            - The data may be too noisy or may not include the right inputs needed to predict the desired outputs. \n",
    "            - This suggests starting over, collecting cleaner data or collecting a richer set of features.\n",
    "    - <font color=\"blue\">If the performance on the training set is acceptable</font>,  \n",
    "        - then <font color=\"blue\">measure the performance on a test set</font>. \n",
    "* <font color=\"blue\">If the performance on the test set is also acceptable</font>,\n",
    "    - then there is nothing left to be done.\n",
    "* <font color=\"purple\">If test set performance is much worse than training set performance</font>,\n",
    "    - then <font color=\"purple\">gathering more data</font> is one of the most eﬀective solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gethering more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The key considerations are the cost and feasibility of gathering moredata, the cost and feasibility of reducing the test error by other means, and the amount of data that is expected to be necessary to improve test set performance signiﬁcantly.\n",
    "* A simple alternative to gathering more data is to \n",
    "    - reduce the size of the model or \n",
    "    - improve regularization, \n",
    "        - by adjusting hyperparameters such as \n",
    "            - weight decay coeﬃcients,or \n",
    "        - by adding regularization strategies such as \n",
    "            - dropout. \n",
    "     - If you ﬁnd that the gap between train and test performance is still unacceptable even after tuning theregularization hyperparameters, \n",
    "         - then gathering more data is advisable.\n",
    "* When deciding whether to gather more data, \n",
    "    - it is also necessary to <font color=\"red\">decide how much to gather</font>. It is helpful to plot curves showing the relationship betweentraining set size and generalization error, like in Fig. 5.4.\n",
    "    - By extrapolating such curves, one can predict how much additional training data would be needed to achieve a certain level of performance. \n",
    "    - Usually, adding a small fraction of the totalnumber of examples will not have a noticeable impact on generalization error. \n",
    "    - It is therefore recommended to experiment with training set sizes on a logarithmic scale,for example doubling the number of examples between consecutive experiments.\n",
    "    <img src=\"http://nbviewer.jupyter.org/github/songorithm/ML/blob/master/part2/study01/dml05/figures/fig5.4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If gathering much more data is not feasible, the only other way to improve generalization error is to <font color=\"red\">improve the learning algorithm itself</font>. \n",
    "* This becomes thedomain of research and not the domain of advice for applied practitioners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.4 Selecting Hyperparameters\n",
    "* 11.4.1 Manual Hyperparameter Tuning\n",
    "* 11.4.2 Automatic Hyperparameter Optimization Algorithms\n",
    "* 11.4.3 Grid Search\n",
    "* 11.4.4 Random Search\n",
    "* 11.4.5 Model-Based Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.svms.org/srm/Sewell2006.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most deep learning algorithms come with many hyperparameters that control many aspects of the algorithm’s behavior. \n",
    "* Some of these hyperparameters aﬀect \n",
    "    - the time and \n",
    "    - memory cost of running the algorithm. \n",
    "* Some of these hyperparameters aﬀect \n",
    "    - the quality of the model recovered \n",
    "        - by the training process and \n",
    "    - its ability to infer correct results \n",
    "        - when deployed on new inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two basic approaches to choosing these hyperparameters: \n",
    "* <font color=\"red\">choosing them manually</font> and \n",
    "* <font color=\"blue\">choosing them automatically</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4.1 Manual Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of manual hyperparameter search is usually to ﬁnd the lowest generalization error subject to some runtime and memory budget."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary goal of manual hyperparameter search is to adjust the eﬀective capacity of the model to match the complexity of the task.\n",
    "* Eﬀective capacityis constrained by three factors: \n",
    "    - the representational capacity of the model, \n",
    "    - the ability of the learning algorithm \n",
    "        - to successfully minimize the cost function\n",
    "            - used to train the model, and \n",
    "    - the degree to which \n",
    "        - the cost function and \n",
    "        - training procedure regularize the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### U-shaped curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generalization error typically follows a U-shaped curve when plotted asa function of one of the hyperparameters, as in Fig. 5.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig5.3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  large hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For some hyperparameters, <font color=\"red\">overﬁtting occurs when the value of the hyperparameter is large</font>. \n",
    "* <font color=\"red\">The number of hidden units</font> in a layer is one such example -> increasing the number of hidden units increases the capacity of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### small hyperparameter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For some hyperparameters, <font color=\"red\">overﬁtting occurs when the value of the hyperparameter is small</font>. \n",
    "* For example, the <font color=\"red\">smallest allowable weight decay coeﬃcient</font> of zero corresponds to the greatest eﬀective capacity of the learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not every hyperparameter will be able to explore the entire U-shaped curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Not every hyperparameter will be able to explore the entire U-shaped curve.\n",
    "* Many <font color=\"red\">hyperparameters are discrete</font>, such as \n",
    "    - the number of units in a layer or \n",
    "    - the number of linear pieces in a maxout unit, \n",
    "    - so it is only possible to <font color=\"red\">visit a few points along the curve</font>. \n",
    "* Some <font color=\"blue\">hyperparameters are binary</font>. \n",
    "    - Usually these hyperparameters are switches that\n",
    "        - specify whether or not to use some optional component of the learning algorithm, such as \n",
    "            - a preprocessing step that \n",
    "                - normalizes the input features by\n",
    "                    - subtracting their mean and\n",
    "                    - dividing by their standard deviation.\n",
    "     - These hyperparameters can <font color=\"blue\">only explore two points on the curve</font>.\n",
    "* Other <font color=\"red\">hyperparameters have</font> \n",
    "    - <font color=\"red\">some minimum</font> or \n",
    "    - <font color=\"red\">maximum value</font> that \n",
    "    - <font color=\"red\">prevents them from exploring some part of the curve</font>. \n",
    "    - For example, \n",
    "        - the minimum weight decay coeﬃcient is zero. \n",
    "        - This means that \n",
    "            - if the model is underﬁtting\n",
    "                - when weight decay is zero, \n",
    "                - we can not enter the overﬁtting region\n",
    "                    - by modifying the weight decay coeﬃcient. \n",
    "         - <font color=\"red\">In other words, some hyperparameters can only subtract capacity</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The learning rate is perhaps the <font color=\"blue\">most important hyperparameter</font>. \n",
    "* <font color=\"red\">If you have time to tune only one hyperparameter, tune the learning rate</font>. \n",
    "* It controls the eﬀective capacity of the model in a <font color=\"red\">more complicated way</font> than other hyperparameters \n",
    "    - the eﬀective capacity of the model is highest \n",
    "        - <font color=\"red\">when</font> the learning rate is <font color=\"red\">correct</font> for the optimization problem, \n",
    "        - <font color=\"red\">not when</font> the learning rate is especially large or especially small. \n",
    "* The learning rate has a U-shaped curve for training error, illustrated in Fig. 11.1. \n",
    "    - When the learning rate is too large, gradient descent can inadvertently increase rather than decrease the training error. \n",
    "    <img src=\"figures/cap11.2.png\" width=600 />\n",
    "    - In the idealized quadratic case, this occurs if the learning rate is at least twice as large as itsoptimal value (LeCun et al., 1998a). \n",
    "    - When the learning rate is too small, trainingis not only slower, but may become permanently stuck with a high training error.\n",
    "        - This eﬀect is poorly understood (it would not happen for a convex loss function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning the parameters other than the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the parameters other than the learning rate requires <font color=\"red\">monitoring both training and test error to diagnose whether your model is overﬁtting or underﬁtting</font>,then adjusting its capacity appropriately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=\"blue\">If your error on the training set is higher than your target error rate</font>,\n",
    "    - you have no choice but to increase capacity. \n",
    "    - If you are \n",
    "        - not using regularization and \n",
    "        - you are conﬁdent that your optimization algorithm is performing correctly, \n",
    "        - then you must \n",
    "            - <font color=\"red\">add more layers</font> to your network or \n",
    "            - <font color=\"red\">add more hidden units</font>. \n",
    "        - Unfortunately, this <font color=\"red\">increases the computational costs</font> associated with the model.\n",
    "* <font color=\"blue\">If your error on the test set is higher than than your target error rate</font>,\n",
    "    - you can now take two kinds of actions. \n",
    "    - The test error is the sum of the training error and the gap between training and test error. \n",
    "        - The optimal test error is found by trading oﬀ these quantities. \n",
    "    - Neural networks typically perform best when \n",
    "        - the training error is very low (and thus, when capacity is high) and \n",
    "        - the test error is primarily driven by the gap between train and test error. \n",
    "    - Your goal is to reduce this gap \n",
    "        - without increasing training error faster than the gap decreases.\n",
    "    - To reduce the gap, \n",
    "        - change regularization hyperparameters \n",
    "            - to reduce eﬀective model capacity, \n",
    "            - such as \n",
    "                - by adding dropout or \n",
    "                - weight decay. \n",
    "        - Usually the best performance comes from a large model that is regularized well, for example by using dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig5.3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap11.3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4.2 Automatic Hyperparameter Optimization Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* http://cs231n.stanford.edu/slides/winter1516_lecture5.pdf\n",
    "* http://scikit-learn.org/stable/modules/grid_search.html\n",
    "* http://scikit-learn.org/stable/auto_examples/model_selection/randomized_search.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual hyperparameter tuning can work very well when the user has a good starting point,such as one determined by others having worked on the same type of applicationand architecture, or when the user has months or years of experience in exploringhyperparameter values for neural networks applied to similar tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However,for many applications, these starting points are not available. <font color=\"red\">In these cases, automated algorithms can ﬁnd useful values of the hyperparameters.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">Unfortunately, hyperparameter  optimization algorithms often have their own hyperparameters</font>, such as the range of values that should be explored for each of the learning algorithm’s hyperparameters. <font color=\"red\">However,these secondary hyperparameters are usually easier to choose</font>, in the sense that acceptable performance may be achieved on a wide range of tasks using the same secondary hyperparameters for all tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4.3 Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there are three or fewer hyperparameters, the common practice is to performgrid search. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For each hyperparameter, the user selects a small ﬁnite set of values to explore. \n",
    "* The grid search algorithm then trains a model for every joint speciﬁcation of hyperparameter values in the Cartesian product of the set of values for each individual hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment that yields the best validation set error is then chosen as having found the best hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the left of Fig. 11.2 for an illustration of a grid of hyperparameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap11.4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### log scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Typically, a grid search involves picking values approximately on a logarithmic scale, e.g., \n",
    "* a learning rate taken within the set \n",
    "    - {.1, .01,$10^{−3}$,$10^{−4}$,$10^{−5}$}, or \n",
    "* a number of hiddenunits taken with the set \n",
    "    - {50, 100, 200, 500, 1000, 2000}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obvious problem with grid search is that its computational cost grows exponentially with the number of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4.4 Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, there is an alternative to grid search that is as simple to program, moreconvenient to use, and converges much faster to good values of the hyperparameters: <font color=\"red\">random search</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap11.4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random search proceeds as follows. \n",
    "* First we deﬁne a marginal distributionfor each hyperparameter,\n",
    "    - e.g., \n",
    "        - a Bernoulli or \n",
    "        - multinoulli for \n",
    "            - binary or \n",
    "            - discrete hyperparameters, or \n",
    "        - a uniform distribution \n",
    "            - on a log-scale for \n",
    "                - positive real-valued hyperparameters. \n",
    "    - For example,\n",
    "        <img src=\"figures/cap11.5.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike in the case of a grid search, \n",
    "* one should not discretizeor bin thevalues of the hyperparameters. \n",
    "* This allows one to explore a larger set of values, and does not incur additional computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main reason why random search ﬁnds good solutions faster than grid searchis that the there are <font color=\"red\">no wasted experimental runs</font>, unlike in the case of grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4.5 Model-Based Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search for good hyperparameters can be cast as an optimization problem. \n",
    "* The decision variables are the hyperparameters. \n",
    "* The cost to be optimized is thevalidation set error that results from training using these hyperparameters. \n",
    "* <font color=\"red\">In simpliﬁed settings where it is feasible to compute the gradient of some diﬀerentiable error measure on the validation set with respect to the hyperparameters, we cansimply follow this gradient</font> (Bengio et al., 1999; Bengio, 2000; Maclaurin et al.,2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">Unfortunately, in most practical settings, this gradient is unavailable, either due to its high computation and memory cost, or due to hyperparameters having intrinsically non-diﬀerentiable interactions with the validation set error, as in thecase of discrete-valued hyperparameters.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">To compensate for this lack of a gradient</font>, \n",
    "* <font color=\"red\">we can build a model of the validation set error</font>, then propose new hyperparameter guesses by performing optimization within this model.\n",
    "* Most model-based algorithms for hyperparameter search use a <font color=\"blue\">Bayesian regression model</font> \n",
    "    - to estimate both the expected value of the validation set error for each hyperparameter and the uncertainty around this expectation.\n",
    "*  Opti-mization thus involves a tradeoﬀ between \n",
    "    - exploration \n",
    "        - (proposing hyperparameters for which there is high uncertainty, which may lead to a large improvement but may also perform poorly) and \n",
    "    - exploitation \n",
    "        - (proposing hyperparameters which the modelis conﬁdent will perform as well as any hyperparameters it has seen so far usually hyperparameters that are very similar to ones it has seen before).\n",
    "* Contemporary approaches to hyperparameter optimization include\n",
    "    - Spearmint (Snoek et al., 2012),\n",
    "    - TPE (Bergstra et al., 2011) and \n",
    "    - SMAC (Hutter et al., 2011)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Currently, we cannot unambiguously recommend Bayesian hyperparameter optimization as an established tool for achieving better deep learning results orfor obtaining those results with less eﬀort.\n",
    "* Bayesian hyperparameter optimizationsometimes performs comparably to human experts, sometimes better, <font color=\"red\">but fails catastrophically on other problems</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameter optimization is an important ﬁeld of research that,\n",
    "* while often driven primarily by the needs of deep learning,\n",
    "* holds the potential to beneﬁt not only the entire ﬁeld of machine learning but the discipline of engineering in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">One drawback common</font> to most hyperparameter optimization algorithms with <font color=\"red\">more sophistication than random search</font> is that they require for a training experiment to run to completion before they are able to extract any informationfrom the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.5 Debugging Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a machine learning system performs poorly, it is usually diﬃcult to tell \n",
    "* whether the poor performance is intrinsic to the algorithm itself or \n",
    "* whether there is a bug in the implementation of the algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning systems arediﬃcult to debug for a variety of reasons.\n",
    "* In most cases, we do not know a priori what the intended behavior of the algorithm is.\n",
    "    - If we train a neural network on a new classiﬁcation task and it achieves 5% test error, we have no straightforward way of knowing if this is the expected behavior or sub-optimal behavior\n",
    "* A further diﬃculty is that most machine learning models have multiple parts that are each adaptive.\n",
    "    - If one part is broken, the other parts can adapt and still achieve roughly acceptable performance.\n",
    "    - For example, \n",
    "        - suppose that we are training a neural net \n",
    "            - with several layers parametrized \n",
    "                - by weights $W$ and biases $b$. \n",
    "        - Suppose further that we have manually implemented \n",
    "            - the gradient descent rule \n",
    "                - for each parameter separately, and \n",
    "            - we made an error in the update for the biases\n",
    "            <img src=\"figures/cap11.6.png\" width=600 />\n",
    "            where $α$ is the learning rate.  \n",
    "        - This erroneous update does not use the gradient at all.\n",
    "        - It causes the biases to constantly become negative throughout learning, which is clearly not a correct implementation of any reasonable learning algorithm. \n",
    "        - The bug may not be apparent just from examining the output of the model though.\n",
    "        - Depending on the distribution of the input, the weights may be able to adapt to compensate for the negative biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### debuggint tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some important debugging tests include:\n",
    "* Visualize the model in action\n",
    "    - When training a model to detect objects inimages, view some images with the detections proposed by the model displayedsuperimposed on the image. \n",
    "    - When training a generative model of speech, listen tosome of the speech samples it produces.\n",
    "* Visualize the worst mistakes\n",
    "    - Most models are able to output some sort of conﬁdence measure for the task they perform.\n",
    "        - For example, classiﬁers based on a softmax output layer assign a probability to each class. \n",
    "        - The probability assigned to the most likely class thus gives an estimate of the conﬁdence the model has inits classiﬁcation decision.\n",
    "    - For example, \n",
    "        - the Street View transcription system originally had a problem wherethe address number detection system would crop the image too tightly and omitsome of the digits. \n",
    "            - The transcription network then assigned very low probability to the correct answer on these images. \n",
    "            - Sorting the images to identify the most conﬁdent mistakes showed that there was a systematic problem with the cropping.\n",
    "* Reasoning about software using train and test error\n",
    "    - It is often diﬃcult to determine whether the underlying software is correctly implemented. \n",
    "    - Some clue scan be obtained from the train and test error. \n",
    "    - If training error is low but test erroris high, \n",
    "        - then it is likely that that the training procedure works correctly, and the model is overﬁtting for fundamental algorithmic reasons. \n",
    "    - An alternative possibilityis that the test error is measured incorrectly due to \n",
    "        - a problem with saving the model after training then reloading it for test set evaluation, or \n",
    "        - if the test data was prepared diﬀerently from the training data.\n",
    "* Fit a tiny dataset\n",
    "    - If you have high error on the training set, determine whether \n",
    "        - it is due to genuine underﬁtting or \n",
    "        - due to a software defect. \n",
    "    - Usually even small models can be guaranteed to be able ﬁt a suﬃciently small dataset.\n",
    "    - For example,\n",
    "        - a classiﬁcation dataset \n",
    "            - with only one example \n",
    "        - can be ﬁt just by setting the biases of the output layer correctly. \n",
    "        - Usually if you cannot train a classiﬁer to correctly label a single example, \n",
    "            - an autoencoder to successfully reproduce a single examplewith high ﬁdelity, or \n",
    "            - a generative model to consistently emit samples resembling a single example, \n",
    "            - there is a software defect preventing successful optimization on the training set. \n",
    "* Compare back-propagated derivatives to numerical derivatives\n",
    "    - If you are using a software framework that requires \n",
    "        - you to implement your own gradient computations, or \n",
    "        - if you are adding a new operation to a diﬀerentiation library and must deﬁne its bprop method, \n",
    "    - then a common source of error is implementing this gradient expression incorrectly.\n",
    "    - One way to verify that these derivatives are correct is \n",
    "        - to compare the derivatives \n",
    "            - computed by your implementation of automatic diﬀerentiation to \n",
    "        - the derivatives computed by a ﬁnite diﬀerences. Because\n",
    "        <img src=\"figures/cap11.7.png\" width=600 />\n",
    "    - If one has access to numerical computation on complex numbers, then there isa very eﬃcient way to numerically estimate the gradient by using complex numbersas input to the function (Squire and Trapp, 1998). The method is based on theobservation that\n",
    "        <img src=\"figures/cap11.8.png\" width=600 />\n",
    "* Monitor histograms of activations and gradient\n",
    "    - It is often useful to visualize statistics of neural network activations and gradients, collected over a large amountof training iterations (maybe one epoch).\n",
    "    - For example, for rectiﬁers,how often are they oﬀ? Are there units that are always oﬀ? \n",
    "* Finally, many deep learning algorithms provide some sort of guarantee about the results produced at each step.\n",
    "    - For example, in Part III, we will see someapproximate inference algorithms that work by using algebraic solutions to optimization problems. \n",
    "        - Typically these can be debugged by testing each of their guarantees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.6 Example: Multi-Digit Number Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* [1] Chapter 11. Practical Methodology(in Bengio's deep learning book) -  http://www.deeplearningbook.org/contents/guidelines.html\n",
    "* [2] Performance measures in Azure ML: Accuracy, Precision, Recall and F1 Score. - https://blogs.msdn.microsoft.com/andreasderuiter/2015/02/09/performance-measures-in-azure-ml-accuracy-precision-recall-and-f1-score/\n",
    "* [3] Using ROC plots and the AUC measure in Azure ML - https://blogs.msdn.microsoft.com/andreasderuiter/2015/02/09/using-roc-plots-and-the-auc-measure-in-azure-ml/\n",
    "* [4] Benchmarking ReLU and PReLU using MNIST and Theano - http://gforge.se/2015/06/benchmarking-relu-and-prelu/\n",
    "* [5] Maxing out the digits - http://fastml.com/maxing-out-the-digits/\n",
    "* [6] L1 : Deep Neural Networks (Udacity) - https://drive.google.com/file/d/0B3vuuoFuJsKWdFFkMS10N1BpLTg/view\n",
    "* [7] Deeplearning4j Updaters Explained - http://deeplearning4j.org/updater\n",
    "* [8] An overview of gradient descent optimization algorithms - http://sebastianruder.com/optimizing-gradient-descent/index.html#adam\n",
    "* [9] Directions in Convolutional Neural Networks at Google - http://vision.stanford.edu/teaching/cs231n/slides/jon_talk.pdf\n",
    "* [10] Batch Normalization (ICML 2015) - http://sanghyukchun.github.io/88/\n",
    "* [11] http://blog.dato.com/how-to-evaluate-machine-learning-models-part-4-hyperparameter-tuning\n",
    "* [12] http://enhancedwiki.altervista.org/en.php?title=Hyperparameter_optimization\n",
    "* [13] http://scikit-learn.org/stable/modules/grid_search.html\n",
    "* [14] http://scikit-learn.org/stable/auto_examples/model_selection/randomized_search.html\n",
    "* [15] http://cs231n.stanford.edu/slides/winter1516_lecture5.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
