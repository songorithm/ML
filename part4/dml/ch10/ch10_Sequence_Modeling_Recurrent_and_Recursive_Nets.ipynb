{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10\n",
    ". Sequence Modeling: Recurrentand Recursive Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 손고리즘ML : 파트 4 - DML [1]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 10.1 Unfolding Computational Graphs\n",
    "* 10.2 Recurrent Neural Networks\n",
    "    - 10.2.1 Computing the Gradient in a Recurrent Neural Network\n",
    "    - 10.2.2 Recurrent Networks as Directed Graphical Models\n",
    "    - 10.2.3 Modeling Sequences Conditioned on Context with RNNs\n",
    "* 10.3 Bidirectional RNNs\n",
    "* 10.4 Encoder-Decoder Sequence-to-Sequence Architectures\n",
    "* 10.5 Deep Recurrent Networks\n",
    "* 10.6 Recursive Neural Networks\n",
    "* 10.7 The Challenge of Long-Term Dependencies\n",
    "* 10.8 Echo State Networks\n",
    "* 10.9 Skip Connections through Time\n",
    "* 10.10 Leaky Units and a Spectrum of Diﬀerent Time Scales\n",
    "* 10.11 The Long Short-Term Memory and Other Gated RNNs\n",
    "    - 10.11.1 LSTM\n",
    "    - 10.11.2 Other Gated RNNs\n",
    "* 10.12 Optimization for Long-Term Dependencies\n",
    "    - 10.12.1 Clipping Gradients\n",
    "* 10.13 Regularizing to Encourage Information Flow\n",
    "* 10.14 Organizing the State at Multiple Time Scales\n",
    "* 10.15 Explicit Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recurrent neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [2] CS224d: Deep Learning for Natural Language Processing : Recurrent neural networks -- for language modeling and other tasks - http://cs224d.stanford.edu/lectures/CS224d-Lecture7.pdf\n",
    "* [3] CS231n: Convolutional Neural Networks for Visual Recognition (2015) - Beyond Image Classification: localization, detection, segmentation - http://vision.stanford.edu/teaching/cs231n/slides/lecture11.pdf \n",
    "Recurrent Networks I: Image Captioning example\n",
    "* [4] CS231n: Convolutional Neural Networks for Visual Recognition (2016) - Recurrent Neural Networks (RNN), Long Short Term Memory (LSTM) - http://cs231n.stanford.edu/slides/winter1516_lecture10.pdf\n",
    " \n",
    "* [5] Probabilistic Graphical Models : Template Models - http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-2-Representation-Template-Models.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent neural networks or RNNs (Rumelhart et al., 1986a) are a family of neural networks for handling sequential data. Sequential data is data where each example consists of a sequence, with each example able to have a different sequence length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter extends the idea of a computational graph, introduced in Sec. 6.5.1, to include cycles. These cycles represent the influence of the present value of a variable on its own value at a future time step. Such computational graphs allow us to define recurrent neural networks. We then describe many different ways to construct, train, and use recurrent neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.1 Unfolding Computational Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this section we explain the idea of unfolding a recursive or recurrent computation into a computational graph that has a repetitive structure, typically corresponding to a chain of events\n",
    "* Unfolding this graph results in the sharing of parameters across a deep network structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### classical form of dynamical system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, consider the classical form of a dynamical system:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where s(t) is called the state of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### unfolding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a finite number of time steps τ, the graph can be unfolded by applying the\n",
    "definition τ − 1 times. For example, if we unfold Eq. 10.1 for τ = 3 time steps, we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.2.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unfolded computational graph of Eq. 10.1 and Eq. 10.2 is illustrated in Fig. 10.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### external signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another example, let us consider a dynamical system driven by an external\n",
    "signal x(t),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we see that the state now contains information about the whole past sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To indicate that the state is the hidden units of\n",
    "the network, we now rewrite Eq. 10.3 using the variable h to represent the state:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.5.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "illustrated in Fig. 10.2, Typical RNNs will add extra architectural features such as\n",
    "output layers that read information out of the state h to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### circuit diagram (recurrent graph) or unfolded computational graph (unrolled graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eq. 10.4 can be drawn in two different ways. \n",
    "* One way to draw the RNN is with a diagram containing one node for every component that might exist in a physical implementation of the model, such as a biological neural network.\n",
    "* The other way to draw the RNN is as an unfolded computational graph, in which each component is represented by many different variables, with one variable per time step, representing the state of the component at that point in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.6.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unfolding process offers two major advantages over simply constructing a\n",
    "function of the full sequence x(t) for t = 1,...,τ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can represent the unfolded recurrence after t steps with a function g(t):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.7.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function g(t) takes the whole past sequence (x(t), x(t−1) , x(t−2), . . . , x(2), x(1) ) as input and produces the current state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another advantage of the unfolding process is that the same function f with <font color=\"red\">the same parameters θ</font> is used at each time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Both the recurrent graph and the unrolled graph have their uses. \n",
    "* The recurrent graph is succint. \n",
    "* The unfolded graph provides an explicit description of which computations to perform. \n",
    "    - The unfolded graph also helps to illustrate the idea of information flow forward in time (computing outputs and losses) and backward in time (computing gradients) by explicitly showing the path along which this information flows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.2 Recurrent Neural Networks\n",
    "   * 10.2.1 Computing the Gradient in a Recurrent Neural Network\n",
    "   * 10.2.2 Recurrent Networks as Directed Graphical Models\n",
    "   * 10.2.3 Modeling Sequences Conditioned on Context with RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples of important design patterns for recurrent neural networks include the following:\n",
    "* Recurrent networks that <font color=\"red\">produce an output at each time step</font> and have <font color=\"blue\">recurrent connections between hidden units</font>, illustrated in Fig. 10.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.8.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Recurrent networks that produce an output at each time step and have <font color=\"red\">recurrent connections only from the output at one time step to the hidden units at the next time step</font>, illustrated in Fig. 10.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.10.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Recurrent networks with recurrent connections between hidden units, that <font color=\"red\">read an entire sequence and then produce a single output</font>, illustrated in Fig. 10.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.12.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### forward propagation equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We now develop the forward propagation equations for the RNN depicted in Fig. 10.3.\n",
    "* Here we assume the hyperbolic tangent activation function.\n",
    "* Here we assume that the output is discrete, as if the RNN is used to predict words or characters.\n",
    "* A natural way to represent discrete variables is to regard the output o as giving the unnormalized log probabilities of each possible value of the discrete variable.\n",
    "* We can then apply the softmax operation as a post-processing step to obtain a vector ˆy of normalized probabilities over the output.\n",
    "* Forward propagation begins with a specification of the initial state h(0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.8.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, for each time step from t = 1 to t = τ, we apply the following update equations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.9.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the parameters are the bias vectors b and c along with the weight matrices U, V and W, respectively for input-to-hidden, hidden-to-output and hidden-to- hidden connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### total loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is an example of a recurrent network that maps an input sequence to an output sequence of the same length. \n",
    "* The total loss for a given sequence of x values paired with a sequence of y values would then be just the sum of the losses over all the time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if L(t) is the negative log-likelihood of y(t) given x(1),...,x(t),then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.11.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Computing the gradient of this loss function with respect to the parameters is an expensive operation.\n",
    "* The runtime is O (τ) and cannot be reduced by parallelization because the forward propagation graph is inherently sequential; each time step may only be computed after the previous one.\n",
    "* States computed in the forward pass must be stored until they are reused during the backward pass, so the memory cost is also O(τ)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### back-propagation through time (BPTT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The back-propagation algorithm applied to the unrolled graph with O(τ) cost is called back-propagation through time or BPTT and is discussed further Sec.\n",
    "10.2.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">The network with recurrence between hidden units is thus very powerful but also expensive to train. Is there an alternative?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network with recurrent connections only from the output at one time step to the hidden units at the next time step (shown in Fig. 10.4) is strictly <font color=\"red\">less powerful</font> because it lacks hidden-to-hidden recurrent connections.\n",
    "* Because this network lacks hidden-to- hidden recurrence, it requires that the output units capture all of the information about the past that the network will use to predict the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">The advantage of eliminating hidden-to-hidden recurrence</font> is is that, for any loss function based on comparing the prediction at time t to the training target at time t , all the time steps are decoupled. Training can thus be parallelized, with the gradient for each step t computed in isolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.10.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### teacher forcing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Models that have recurrent connections from their outputs leading back into the model may be trained with teacher forcing.\n",
    "* Teacher forcing is a procedure that emerges from the maximum likelihood criterion, in which during training the model receives the ground truth output y(t) as input at time t+ 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this by examining a sequence with two time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conditional maximum likelihood criterion is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.13.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.14.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We originally motivated teacher forcing as allowing us to avoid back-propagation through time in models that lack hidden-to-hidden connections. \n",
    "* Teacher forcing may still be applied to models that have hidden-to-hidden connections so long as they have connections from the output at one time step to values computed in the next time step. \n",
    "* However, as soon as the hidden units become a function of earlier time steps, the BPTT algorithm is necessary. Some models may thus be trained with both teacher forcing and BPTT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The disadvantage of strict teacher forcing arises if the network is going to be later used in an open-loop mode, with the network outputs (or samples from the output distribution) fed back as input.\n",
    "* In this case, the kind of inputs that the network sees during training could be quite different from the kind of inputs that it will see at test time.\n",
    "* One way to mitigate this problem is to train with both teacher-forced inputs and with free-running inputs, for example by predicting the correct target a number of steps in the future through the unfolded recurrent output-to-input paths. \n",
    "    - In this way, the network can learn to take into account input conditions (such as those it generates itself in the free-running mode) not seen during training and how to map the state back towards one that will make the network generate proper outputs after a few steps. \n",
    "* Another approach (Bengio et al., 2015b) to mitigate the gap between the inputs seen at train time and the inputs seen at test time randomly chooses to use generated values or actual data values as input. \n",
    "    - This approach exploits a curriculum learning strategy to gradually use more of the generated values as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2.1 Computing the Gradient in a Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the gradient through a recurrent neural network is straightforward. One simply applies the generalized back-propagation algorithm of Sec. 6.5.6 to the unrolled computational graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of back-propagation on the unrolled graph is called <font color=\"red\">the back-propagation through time (BPTT) algorithm</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to compute gradients by BPTT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gain some intuition for how the BPTT algorith behaves, we provide an example of how to compute gradients by BPTT for the RNN equations above (Eqs. 10.6 and 10.7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.9.png\" width=600 />\n",
    "<img src=\"figures/cap10.11.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.8.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each node N we need to compute the gradient ∇NL recursively, based on the gradient computed at nodes that follow it in the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start the recursion with the nodes immediately preceding the final loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.15.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient ∇o(t)L on the outputs at time step t, for all i,t, is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.16.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the final time step τ, h (τ) only has o(τ) as a descendent, so its gradient is simple:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.17.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then iterate backwards in time to back-propagate gradients through time, from t = τ − 1 down to t = 1, noting that h(t) (for t < τ ) has as descendents both o(t) and h(t+1). Its gradient is thus given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.18.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where diag(1 − (h(t+1)^2) indicates the diagonal matrix containing the elements 1 − (h_i(t+1))^2 . This is the Jacobian of the hyperbolic tangent associated with the hidden unit i at time t + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the gradients on the internal nodes of the computational graph are obtained, we can obtain the gradients on the parameter nodes, which have descendents at all the time steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.19.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2.2 Recurrent Networks as Directed Graphical Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As with a feedforward network, we usually wish to interpret the output of the RNN as a probability distribution, and we usually use the cross-entropy associated with that distribution to define the loss. \n",
    "* Mean squared error is the cross-entropy loss associated with an output distribution that is a unit Gaussian, for example, just as with a feedforward network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.8.png\" width=600 />\n",
    "<img src=\"figures/cap10.9.png\" width=600 />\n",
    "<img src=\"figures/cap10.11.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use a predictive log-likelihood training objective, such as Eq. 10.7, we train the RNN to estimate the conditional distribution of the next sequence element y(t) given the past inputs. This may mean that we maximize the log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.20.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or, if the model includes connections from the output at one time step to the next\n",
    "time step,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.21.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Decomposing the joint probability over the sequence of y values as a series of one-step probabilistic predictions is one way to capture the full joint distribution across the whole sequence. \n",
    "* When we do not feed past y values as inputs that condition the next step prediction, the directed graphical model contains no edges from any y(i) in the past to the current y(t). In this case, the outputs y are conditionally independent given the sequence of x values. \n",
    "* When we do feed the actual y values (not their prediction, but the actual observed or generated values) back into the network, the directed graphical model contains edges from all y(i) values in the past to the current y (t) value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As a simple example, \n",
    "\n",
    "let us consider the case where the RNN models only a sequence of scalar random variables Y = { y(1), . . . , y(τ) } , with no additional inputs x. The input at time step t is simply the output at time step t − 1. The RNN then defines a directed graphical model over the y variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We parametrize the joint distribution of these observations using the chain rule (Eq. 3.6) for conditional probabilities:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.23.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the right-hand side of the bar is empty for t = 1, of course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence the negative log-likelihood of a set of values { y(1), . . . , y(τ) } according to such a model is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.24.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Many graphical models aim to achieve statistical and computational efficiency by omitting edges that do not correspond to strong interactions.\n",
    "* For example, it is common to make the Markov assumption that the graphical model should only contain edges from {y(t−k) , . . . , y(t−1)} to y(t), rather than containing edges from the entire past history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### all past inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* However, in some cases, we believe that all past inputs should have an influence on the next element of the sequence. \n",
    "* RNNs are useful when we believe that the distribution over y(t) may depend on a value of y(i) from the distant past in a way that is not captured by the effect of y(i) on y(t−1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ignoring the hidden units\n",
    "* The graphical model over the y values with the complete graph structure is shown in Fig. 10.7. \n",
    "* The complete graph interpretation of the RNN is based on ignoring the hidden units h(t) by marginalizing them out of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.22.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### including the hidden units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The hidden units are a deterministic function of their inputs, so it may seem unusual to regard them as random variables. \n",
    "* However, it is perfectly legitimate to regard them as random variables. \n",
    "* The RNN defines conditional probability distribution over the hidden units given their inputs; the conditional distribution just happens to be a probability distribution that assigns probability 1 to a single state and probability 0 to all other states. \n",
    "* Including the hidden units in the graphical model reveals that the RNN provides a very efficient parametrization of the joint distribution over the observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.25.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### some computationally challenging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Even with the efficient parametrization of the graphical model, some operations remain computationally challenging. \n",
    "* For example, it is difficult to predicting missing values in the middle of the sequence.\n",
    "* The statistical complexity of the RNN may be flexibly adjusted by changing the complexity of the function f(h(t−1);x(t)) that produces the state variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The graphical model of recurrent networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The graphical model of recurrent networks is directed and decomposes the probability distribution as a product of conditionals without explicitly cutting any arc in the graphical model.\n",
    "* The price recurrent networks pay for their reduced number of parameters is that <font color=\"red\">optimizing the parameters</font> may be difficult.\n",
    "* The <font color=\"red\">parameter sharing</font> used in recurrent networks relies on the assumption that the same parameters can be used for different time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete our view of an RNN as a graphical model, we must describe how to draw samples from the model.\n",
    "* The main operation that we need to perform is simply to sample from the conditional distribution at each time step. \n",
    "* However, there is one additional complication. \n",
    "* The RNN must have some mechanism for determining the length of the sequence. This can be achieved in various ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.26.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2.3 Modeling Sequences Conditioned on Context with RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.8.png\" width=600 />\n",
    "<img src=\"figures/cap10.9.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.20.png\" width=600 />\n",
    "<img src=\"figures/cap10.21.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.22.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.25.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the previous section we described how an RNN could correspond to a directed graphical model over a sequence of random variables y(t) with no inputs x.\n",
    "* In general, RNNs allow the extension of the graphical model view to represent not only a joint distribution over the y variables but also a conditional distribution over y given x.\n",
    "* As discussed in the context of feedforward networks in Sec. 6.2.1.1, any model representing a variable P (y;θ) can be reinter- preted as model representing a conditional distribution P(y|ω ) with ω= θ.\n",
    "* We can extend such a model to represent a distribution P(y | x) by using the same P(y | ω) as before, but making ω a function of x. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fixed-size inputs\n",
    "* Previously, we have discussed RNNs that take a sequence of vectors x(t) for t = 1,...,τ as input.\n",
    "* Another option is to take only a single vector x as input. When x is a fixed-size vector, we can simply make it an extra input of the RNN that generates the y sequence.\n",
    "* Some common ways of providing an extra input to an RNN are:\n",
    "    1. as an extra input at each time step, or \n",
    "    2. as the initial state h(0), or\n",
    "    3. both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first and most common approach is illustrated in Fig. 10.9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.28.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a sequence of vectors x(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than receiving only a single vector x as input, the RNN may receive a sequence of vectors x(t) as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conditional independence assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.27.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### to remove conditional independence assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To remove the conditional independence assumption, we can add connections from the output at time t to the hidden unit at time t+ 1, as shown in Fig. 10.10.\n",
    "* This kind of model representing a distribution over a sequence given another sequence still has one restriction, which is that the length of both sequences must be the same. We describe how to remove this restriction in Sec. 10.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.29.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.3 Bidirectional RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All of the recurrent networks we have considered up to now have a “causal” structure, meaning that the state at time t only captures information from the past, x(1), . . . , x(t−1), and the present input x(t) .\n",
    "* Some of the models we have discussed also allow information from past y values to affect the current state when the y values are available.\n",
    "* However, in many applications we want to output a prediction of y(t) which may depend on the whole input sequence.\n",
    "     - For example, in speech recognition, the correct interpretation of the current sound as a phoneme may depend on the next few phonemes because of co-articulation and potentially may even depend on the next few words because of the linguistic dependencies between nearby words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bidirectional recurrent neural networks (or bidirectional RNNs) were invented\n",
    "to address that need (Schuster and Paliwal, 1997). They have been extremely suc- cessful (Graves, 2012) in applications where that need arises, such as handwriting recognition (Graves et al., 2008; Graves and Schmidhuber, 2009), speech recogni- tion (Graves and Schmidhuber, 2005; Graves et al., 2013) and bioinformatics (Baldi\n",
    "et al., 1999).\n",
    "* As the name suggests, bidirectional RNNs combine an RNN that moves forward\n",
    "through time beginning from the start of the sequence with another RNN that moves backward through time beginning from the end of the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.30.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This idea can be naturally extended to 2-dimensional input, such as images,by having four RNNs, each one going in one of the four directions: up, down, left, right.\n",
    "* Compared to a convolutional network, RNNs applied to images are typically more expensive but allow for long-range lateral interactions between features in the same feature map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.4 Encoder-Decoder Sequence-to-Sequence Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we discuss how an RNN can be trained to map an input sequence to an output sequence which is not necessarily of the same length.\n",
    "* This comes up in many applications, such as speech recognition, machine translation or question answering, where the input and output sequences in the training set are generally not of the same length (although their lengths might be related)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### context\n",
    "* We often call the input to the RNN the “context.” We want to produce a representation of this context, C. The context C might be a vector or sequence of vectors that summarize the input sequence X = (x(1), . . . , x(nx ) )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the encoder-decoder or sequence-to-sequence architecture\n",
    "* The simplest RNN architecture for mapping a variable-length sequence to another variable-length sequence was first proposed by Cho et al. (2014a) and shortly after by Sutskever et al. (2014b).\n",
    "* These authors respectively called this architecture, illustrated in Fig. 10.12, the encoder-decoder or sequence-to-sequence architecture.\n",
    "* The idea is very simple: \n",
    "    - (1) an encoder or reader or input RNN processes the input sequence. \n",
    "        - The encoder emits the context C , usually as a simple function of its final hidden state. \n",
    "    - (2) a decoder or writer or output RNN is conditioned on that fixed-length vector (just like in Fig. 10.9) to generate the output sequence Y = (y(1) , . . . , y(ny) ). \n",
    "* In a sequence-to-sequence architecture, the two RNNs are trained jointly to maximize the average of logP(y(1),...,y(ny) |x(1),...,x(nx)) over all the pairs of x and y sequences in the training set. \n",
    "* The last state h_nx of the encoder RNN is typically used as a representation C of the input sequence that is provided as input to the decoder RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.31.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If the context C is a vector, then the decoder RNN is simply a vector-to- sequence RNN as described in Sec. 10.2.3.\n",
    "* One clear limitation of this architecture is when the context C output by the encoder RNN has a dimension that is too small to properly summarize a long sequence. \n",
    "    - This phenomenon was observed by Bahdanau et al. (2014) in the context of machine translation. \n",
    "    - They proposed to make C a variable-length sequence rather than a fixed-size vector. \n",
    "    - Additionally, they introduced an attention mechanism that learns to associate elements of the sequence C to elements of the output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.5 Deep Recurrent Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The computation in most RNNs can be decomposed into three blocks of parameters and associated transformations:\n",
    "    1. from the input to the hidden state,\n",
    "    2. from the previous hidden state to the next hidden state, and \n",
    "    3. from the hidden state to the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With the RNN architecture of Fig. 10.3, each of these three blocks is associated with a single weight matrix. In other words, when the network is unfolded, each of these corresponds to a shallow transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Would it be advantageous to introduce depth in each of these operations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimental evidence (Graves et al., 2013; Pascanu et al., 2014a) strongly suggests so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### deep RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Graves et al. (2013) were the first to show a significant benefit of decomposing the state of an RNN into multiple layers as in Fig. 10.13 (left).\n",
    "    - We can think of the lower layers in the hierarchy depicted in Fig. 10.13a as playing a role in transforming the raw input into a representation that is more appropriate, at the higher levels of the hidden state.\n",
    "* Pascanu et al. (2014a) go a step further and propose to have a separate MLP (possibly deep) for each of the three blocks enumerated above, as illustrated in Fig. 10.13b.\n",
    "    - Considerations of representational capacity suggest to allocate enough capacity in each of these three steps, but doing so by adding depth may hurt learning by making optimization difficult.\n",
    "    - In general, it is easier to optimize shallower architectures, and adding the extra depth of Fig. 10.13b makes the shortest path from a variable in time step t to a variable in time step t + 1 become longer.\n",
    "* However, as argued by Pascanu et al. (2014a), this can be mitigated by introducing skip connections in the hidden-to-hidden path, as illustrated in Fig. 10.13c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.32.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.6 Recursive Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.33.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.7 The Challenge of Long-Term Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.34.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.35.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.8 Echo State Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.9 Skip Connections through Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.10 Leaky Units and a Spectrum of Diﬀerent Time Scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.11 The Long Short-Term Memory and Other Gated RNNs\n",
    "   * 10.11.1 LSTM\n",
    "   * 10.11.2 Other Gated RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.11.1 LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.36.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.37.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.38.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.11.2 Other Gated RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.40.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.41.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.12 Optimization for Long-Term Dependencies\n",
    "   * 10.12.1 Clipping Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.42.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.12.1 Clipping Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.43.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.44.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.13 Regularizing to Encourage Information Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"figures/cap10.45.png\" width=600 />\n",
    "<img src=\"figures/cap10.46.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.14 Organizing the State at Multiple Time Scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.15 Explicit Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료\n",
    "* [1] Bengio's deep learning book / Chapter 10. Sequence Modeling: Recurrentand Recursive Nets - http://www.deeplearningbook.org/contents/rnn.html\n",
    "* [2] CS224d: Deep Learning for Natural Language Processing : Recurrent neural networks -- for language modeling and other tasks - http://cs224d.stanford.edu/lectures/CS224d-Lecture7.pdf\n",
    "* [3] CS231n: Convolutional Neural Networks for Visual Recognition (2015) - Beyond Image Classification: localization, detection, segmentation - http://vision.stanford.edu/teaching/cs231n/slides/lecture11.pdf \n",
    "Recurrent Networks I: Image Captioning example\n",
    "* [4] CS231n: Convolutional Neural Networks for Visual Recognition (2016) - Recurrent Neural Networks (RNN), Long Short Term Memory (LSTM) - http://cs231n.stanford.edu/slides/winter1516_lecture10.pdf\n",
    " \n",
    "* [5] Probabilistic Graphical Models : Template Models - http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-2-Representation-Template-Models.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
