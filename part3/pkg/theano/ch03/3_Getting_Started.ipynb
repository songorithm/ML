{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 손고리즘 / 손고리즘ML : 파트 3 - theano [1]\n",
    "* 김무성    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3.1 Download\n",
    "* 3.2 Datasets\n",
    "    - 3.2.1 MNIST Dataset\n",
    "* 3.3 Notation\n",
    "    - 3.3.1 Dataset notation\n",
    "    - 3.3.2 Math Conventions\n",
    "    - 3.3.3 List of Symbols and acronyms\n",
    "* 3.4 A Primer on Supervised Optimization for Deep Learning\n",
    "    - 3.4.1 Learning a Classifier\n",
    "        - Zero-One Loss\n",
    "        - Negative Log-Likelihood Loss\n",
    "    - 3.4.2 Stochastic Gradient Descent\n",
    "    - 3.4.3 Regularization\n",
    "        - L1 and L2 regularization\n",
    "        - Early-Stopping\n",
    "    - 3.4.4 Testing\n",
    "    - 3.4.5 Recap\n",
    "* 3.5 Theano/Python Tips\n",
    "    - 3.5.1 Loading and Saving Models\n",
    "        - Pickle the numpy ndarrays from your shared variables\n",
    "        - Do not pickle your training or test functions for long-term storage\n",
    "    - 3.5.2 Plotting Intermediate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docker pull jupyter/scipy-notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://github.com/jupyter/docker-stacks/tree/master/scipy-notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docker run -d -p 8888:8888 -e GRANT_SUDO=yes --name run_theano jupyter/scipy-notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docker-machine ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "http://ip:port/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "현재는 python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "python2로 바꾸려면 source activate python2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conda install theano (파이썬 2와 파이썬 3에 각각 해주자 - 안쓸 버전엔 안해도 되긴함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* git clone git://github.com/lisa-lab/DeepLearningTutorials.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE.txt  README.rst  code  data  doc  issues_closed  issues_open  misc\r\n"
     ]
    }
   ],
   "source": [
    "!ls ~/work/DeepLearningTutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBN.py\t\t      dA.py\t\t  logistic_cg.py   rbm.py     utils.py\r\n",
      "SdA.py\t\t      hmc\t\t  logistic_sgd.py  rnnrbm.py\r\n",
      "cA.py\t\t      imdb.py\t\t  lstm.py\t   rnnslu.py\r\n",
      "convolutional_mlp.py  imdb_preprocess.py  mlp.py\t   test.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls ~/work/DeepLearningTutorials/code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3.2.1 MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1 MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap3.1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* mnist.pkl.gz - http://deeplearning.net/data/mnist/mnist.pkl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3_Getting_Started.ipynb  figures\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted 'http://deeplearning.net/data/mnist/mnist.pkl.gz' (ANSI_X3.4-1968) -> 'http://deeplearning.net/data/mnist/mnist.pkl.gz' (UTF-8)\n",
      "--2015-11-23 04:23:26--  http://deeplearning.net/data/mnist/mnist.pkl.gz\n",
      "Resolving deeplearning.net (deeplearning.net)... 132.204.26.28\n",
      "Connecting to deeplearning.net (deeplearning.net)|132.204.26.28|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16168813 (15M) [application/x-gzip]\n",
      "Saving to: 'mnist.pkl.gz'\n",
      "\n",
      "mnist.pkl.gz        100%[=====================>]  15.42M   484KB/s   in 50s    \n",
      "\n",
      "2015-11-23 04:24:21 (319 KB/s) - 'mnist.pkl.gz' saved [16168813/16168813]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://deeplearning.net/data/mnist/mnist.pkl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle, gzip, numpy\n",
    "# Load the dataset\n",
    "f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = cPickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shared_dataset(data_xy):\n",
    "    \"\"\" Function that loads the dataset into shared variables\n",
    "    \n",
    "    The reason we store our dataset in shared variables is to allow\n",
    "    Theano to copy it into the GPU memory (when code is run on GPU).\n",
    "    Since copying data into the GPU is slow, copying a minibatch everytime\n",
    "    is needed (the default behaviour if the data is not in a shared\n",
    "    variable) would lead to a large decrease in performance.\n",
    "    \"\"\"\n",
    "    data_x, data_y = data_xy\n",
    "    shared_x = theano.shared(numpy.asarray(data_x, dtype=theano.config.floatX)) \n",
    "    shared_y = theano.shared(numpy.asarray(data_y, dtype=theano.config.floatX))\n",
    "    # When storing data on the GPU it has to be stored as floats\n",
    "    # therefore we will store the labels as ‘‘floatX‘‘ as well\n",
    "    # (‘‘shared_y‘‘ does exactly that). But during our computations\n",
    "    # we need them as ints (we use labels as index, and if they are\n",
    "    # floats it doesn’t make sense) therefore instead of returning\n",
    "    # ‘‘shared_y‘‘ we will have to cast it to int. This little hack\n",
    "    # lets us get around this issue\n",
    "    return shared_x, T.cast(shared_y, 'int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "test_set_x, test_set_y = shared_dataset(test_set)\n",
    "valid_set_x, valid_set_y = shared_dataset(valid_set)\n",
    "train_set_x, train_set_y = shared_dataset(train_set)\n",
    "\n",
    "batch_size = 500 # size of the minibatch\n",
    "\n",
    "# accessing the third minibatch of the training set\n",
    "data  = train_set_x[2 * 500: 3 * 500]\n",
    "label = train_set_y[2 * 500: 3 * 500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3.3.1 Dataset notation\n",
    "* 3.3.2 Math Conventions\n",
    "* 3.3.3 List of Symbols and acronyms\n",
    "* 3.3.4 Python Namespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.1 Dataset notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.2 Math Conventions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap3.2.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap3.3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.3 List of Symbols and acronyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap3.4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.4 Python Namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 A Primer on Supervised Optimization for Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3.4.1 Learning a Classifier\n",
    "* 3.4.2 Stochastic Gradient Descent\n",
    "* 3.4.3 Regularization \n",
    "* 3.4.5 Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.1 Learning a Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Zero-One Loss\n",
    "* Negative Log-Likelihood Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-One Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고 :\n",
    "\n",
    "<img src=\"http://fa.bianp.net/talks/trento_may_2015/img/logistic.svg\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap3.5.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap3.6.png\" width=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# zero_one_loss is a Theano variable representing a symbolic\n",
    "# expression of the zero one loss ; to get the actual value this\n",
    "# symbolic expression has to be compiled into a Theano function (see # the Theano tutorial for more details)\n",
    "zero_one_loss = T.sum(T.neq(T.argmax(p_y_given_x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Log-Likelihood Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap3.7.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap3.8.png\" width=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NLL is a symbolic variable ; to get the actual value of NLL, this symbolic # expression has to be compiled into a Theano function (see the Theano\n",
    "# tutorial for more details)\n",
    "NLL = -T.sum(T.log(p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "# note on syntax: T.arange(y.shape[0]) is a vector of integers [0,1,2,...,len(y)].\n",
    "# Indexing a matrix M by the two vectors [0,1,...,K], [a,b,...,k] returns the\n",
    "# elements M[0,a], M[1,b], ..., M[K,k] as a vector.  Here, we use this\n",
    "# syntax to retrieve the log-probability of the correct labels, y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.2 Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고 [3]\n",
    "\n",
    "- http://vision.stanford.edu/teaching/cs231n/slides/lecture4.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADIENT DESCENT\n",
    "while True:\n",
    "    loss = f(params)\n",
    "    d_loss_wrt_params = ... # compute gradient \n",
    "    params -= learning_rate * d_loss_wrt_params \n",
    "    if <stopping condition is met>:\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STOCHASTIC GRADIENT DESCENT\n",
    "for (x_i,y_i) in training_set:\n",
    "                            # imagine an infinite generator\n",
    "                            # that may repeat examples (if there is only a finite training\n",
    "    loss = f(params, x_i, y_i) \n",
    "    d_loss_wrt_params = ... # compute gradient \n",
    "    params -= learning_rate * d_loss_wrt_params \n",
    "    if <stopping condition is met>:\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for (x_batch,y_batch) in train_batches:\n",
    "                            # imagine an infinite generator\n",
    "                            # that may repeat examples\n",
    "    loss = f(params, x_batch, y_batch)\n",
    "    d_loss_wrt_params = ... # compute gradient using theano \n",
    "    params -= learning_rate * d_loss_wrt_params\n",
    "    if <stopping condition is met>: \n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minibatch Stochastic Gradient Descent\n",
    "\n",
    "# assume loss is a symbolic description of the loss function given\n",
    "# the symbolic variables params (shared variable), x_batch, y_batch;\n",
    "\n",
    "# compute gradient of loss with respect to params\n",
    "d_loss_wrt_params = T.grad(loss, params)\n",
    "\n",
    "# compile the MSGD step into a theano function\n",
    "updates = [(params, params - learning_rate * d_loss_wrt_params)]\n",
    "MSGD = theano.function([x_batch,y_batch], loss, updates=updates)\n",
    "\n",
    "for (x_batch, y_batch) in train_batches:\n",
    "    # here x_batch and y_batch are elements of train_batches and\n",
    "    # therefore numpy arrays; function MSGD also updates the params \n",
    "    print('Current loss is ', MSGD(x_batch, y_batch))\n",
    "    if stopping_condition_is_met:\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.3 Regularization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* L1 and L2 regularization\n",
    "* Early-Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 and L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap3.9.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고 [2] : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/songorithm/ML/raw/f9f2c631f14613f5051eed28f70ffeb130d9c219/part2/study04/dml07/figures/cap7.21.png\" width=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# symbolic Theano variable that represents the L1 regularization term\n",
    "L1 = T.sum(abs(param))\n",
    "\n",
    "# symbolic Theano variable that represents the squared L2 term\n",
    "L2_sqr = T.sum(param ** 2)\n",
    "\n",
    "# the loss\n",
    "loss = NLL + lambda_1 * L1 + lambda_2 * L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early-Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고 [2] : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/songorithm/ML/raw/f9f2c631f14613f5051eed28f70ffeb130d9c219/part2/study04/dml07/figures/cap7.44.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/songorithm/ML/raw/f9f2c631f14613f5051eed28f70ffeb130d9c219/part2/study04/dml07/figures/cap7.48.png\" width=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# early-stopping parameters\n",
    "patience = 5000 # look as this many examples regardless \n",
    "patience_increase = 2 # wait this much longer when a new best is\n",
    "                        # found\n",
    "improvement_threshold = 0.995 # a relative improvement of this much is\n",
    "                               # considered significant\n",
    "validation_frequency = min(n_train_batches, patience/2) # go through this many\n",
    "                              # minibatches before checking the network\n",
    "                              # on the validation set; in this case we\n",
    "                              # check every epoch\n",
    "best_params = None\n",
    "best_validation_loss = numpy.inf\n",
    "test_score = 0.\n",
    "start_time = time.clock()\n",
    "\n",
    "done_looping = False\n",
    "epoch = 0\n",
    "\n",
    "while (epoch < n_epochs) and (not done_looping):\n",
    "    # Report \"1\" for first epoch, \"n_epochs\" for last epoch\n",
    "    epoch = epoch + 1\n",
    "    for minibatch_index in xrange(n_train_batches):\n",
    "\n",
    "        d_loss_wrt_params = ... # compute gradient\n",
    "        params -= learning_rate * d_loss_wrt_params # gradient descent\n",
    "        \n",
    "        # iteration number. We want it to start at 0.\n",
    "        iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "        # note that if we do ‘iter % validation_frequency‘ it will be \n",
    "        # true for iter = 0 which we do not want. We want it true for \n",
    "        # iter = validation_frequency - 1.\n",
    "        if (iter + 1) % validation_frequency == 0:\n",
    "\n",
    "            this_validation_loss = ... # compute zero-one loss on validation set \n",
    "            \n",
    "            if this_validation_loss < best_validation_loss:\n",
    "                                       \n",
    "                # improve patience if loss improvement is good enough\n",
    "                if this_validation_loss < best_validation_loss * improvement_threshold:\n",
    "                    patience = max(patience, iter * patience_increase)\n",
    "                \n",
    "                best_params = copy.deepcopy(params)\n",
    "                best_validation_loss = this_validation_loss\n",
    "\n",
    "        if patience <= iter:\n",
    "            done_looping = True\n",
    "            break\n",
    "\n",
    "# POSTCONDITION :\n",
    "# best_params refers to the best out-of-sample parameters observed during the optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.4 Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.5 Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 Theano/Python Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3.5.1 Loading and Saving Models\n",
    "* 3.5.2 Plotting Intermediate Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5.1 Loading and Saving Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pickle the numpy ndarrays from your shared variables\n",
    "* Do not pickle your training or test functions for long-term storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle the numpy ndarrays from your shared variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "save_file = open('path', 'wb') # this will overwrite current contents\n",
    "cPickle.dump(w.get_value(borrow=True), save_file, -1) # the -1 is for HIGHEST_PROTOCOL \n",
    "cPickle.dump(v.get_value(borrow=True), save_file, -1) # .. and it triggers much more e \n",
    "cPickle.dump(u.get_value(borrow=True), save_file, -1) # .. storage than numpy’s defaul \n",
    "save_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_file = open('path')\n",
    "w.set_value(cPickle.load(save_file), borrow=True) \n",
    "v.set_value(cPickle.load(save_file), borrow=True) \n",
    "u.set_value(cPickle.load(save_file), borrow=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do not pickle your training or test functions for long-term storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5.2 Plotting Intermediate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3_Getting_Started.ipynb  figures  logistic_sgd.py  mnist.pkl.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1] Deep Learning Tutorial - http://deeplearning.net/tutorial/deeplearning.pdf\n",
    "* [2] \n",
    "* [3] Optimization, higher-level representations, image features - "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
