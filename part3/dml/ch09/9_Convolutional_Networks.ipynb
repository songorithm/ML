{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Chapter 9 Convolutional Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 손고리즘 / 손고리즘 ML : 파트 3 - DML [1]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 9.1 The Convolution Operation\n",
    "* 9.2 Motivation\n",
    "* 9.3 Pooling\n",
    "* 9.4 Convolution and Pooling as an Inﬁnitely Strong Prior\n",
    "* 9.5 Variants of the Basic Convolution Function\n",
    "* 9.6 Structured Outputs\n",
    "* 9.7 Convolutional Modules\n",
    "* 9.8 Data Types\n",
    "* 9.9 Eﬃcient Convolution Algorithms\n",
    "* 9.10 Random or Unsupervised Features\n",
    "* 9.11 The Neuroscientiﬁc Basis for Convolutional Networks\n",
    "* 9.12 Convolutional Networks and the History of DeepLearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional networks (also known as convolutional neural networks or CNNs)are a specialized kind of neural network for processing data that has a known,grid-like topology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name “convolutional neural network”indicates that the network employs a mathematical operation called convolution. Convolution is a specialized kind of linear operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional networksare simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.1 The Convolution Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolution operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Suppose we are tracking the location of a spaceship with a laser sensor. Ourlaser sensor provides a single output x(t), the position of the spaceship at timet. \n",
    "* Now suppose that our laser sensor is somewhat noisy. To obtain a less noisyes timate of the spaceship’s position, we would like to average together several measurements. \n",
    "* Of course, more recent measurements are more relevant, so wewill want this to be a weighted average that gives more weight to recent measurements.\n",
    "* We can do this with a weighting function w(a), where a is the age of ameasurement. If we apply such a weighted average operation at every moment,we obtain a new function s providing a smoothed estimate of the position of thespaceship:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolution operation is typically denoted with an asterisk:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.2.png\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### input & kernel & feature map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In convolutional network terminology, the ﬁrst argument (in this example,the function x) to the convolution is often referred to as the input and the secondargument (in this example, the function w) as the kernel. The output is sometimes referred to as the feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### discrete convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.3.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multidimensional case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning applications, the input is usually a multidimensional arrayof data and the kernel is usually a multidimensional array of learn-able parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will refer to these multidimensional arrays as tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if we use a two-dimensional image I as our input, we probably alsowant to use a two-dimensional kernel K :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.4.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that convolution is commutative, meaning we can equivalently write:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.5.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cross-correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the commutative property is useful for writing proofs, it is not usuallyan important property of a neural network implementation. Instead, many neuralnetwork libraries implement a related function called the cross-correlation, whichis the same as convolution but without ﬂipping the kernel:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.6.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Many machine learning libraries implement cross-correlation but call it convolution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.7.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discrete convolution can be viewed as multiplication by a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing convolution as matrix multiplication usually does nothelp to implement convolution operations, but it is useful for understanding anddesigning neural networks.\n",
    "* Any neural network algorithm that works with matrix multiplication and does not depend on speciﬁc properties of the matrix structure should work with convolution, without requiring any further changes to the neuralnetwork. \n",
    "* Typical convolutional neural networks do make use of further specializations in order to deal with large inputs eﬃciently, but these are not strictly necessary from a theoretical perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.2 Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution leverages three important ideas that can help improve a machinelearning system: \n",
    "* sparse interactions, \n",
    "* parameter sharing, and \n",
    "* equivariant representations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.8.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.9.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.10.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.11.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.12.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.3 Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical layer of a convolutional network consists of three stages (see Fig. 9.7).\n",
    "* In the ﬁrst stage, the layer performs several convolutions in parallel to produce aset of presynaptic activations. \n",
    "* In the second stage, each presynaptic activation isrun through a nonlinear activation function, such as the rectiﬁed linear activationfunction. This stage is sometimes called the detector stage. \n",
    "* In the third stage,we use a pooling function to modify the output of the layer further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pooling function \n",
    "* replaces the output of the net at a certain location with asummary statistic of the nearby outputs. \n",
    "* For example, \n",
    "    - the max pooling operation \n",
    "        - reports the maximum output within a rectangular neighborhood. \n",
    "* Other popular pooling functions include \n",
    "    - the average of a rectangular neighborhood, \n",
    "    - the L2 norm of a rectangular neighborhood, or \n",
    "    - a weighted average based on the distance from the central pixe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.13.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.14.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### translation invariant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all cases, pooling helps to make the representation become invariant to small translations of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KEY IDEA : Invariance to local translationcan be a very useful property if we care more about whether somefeature is present than exactly where it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.15.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inﬁnitely strong prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of pooling can be viewed as adding an inﬁnitely strong prior thatthe function the layer learns must be invariant to small translations. When thisassumption is correct, it can greatly improve the statistical eﬃciency of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transformation invariant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling over spatial regions produces invariance to translation, but if we pool over the outputs of separately parametrized convolutions, the features can learn which transformations to become invariant to (see Fig. 9.9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.16.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pooling with downsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.17.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.4 Convolution and Pooling as an Inﬁnitely Strong Prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* An inﬁnitely strong prior places zero probability on some parameters and says that these parameter values are completely forbidden, regardless of how much support the data gives to those values.\n",
    "* Of course, implementing a convolutional net as a fully connected net with aninﬁnitely strong prior would be extremely computationally wasteful. But thinkingof a convolutional net as a fully connected net with an inﬁnitely strong prior cangive us some insights into how convolutional nets work.\n",
    "* One key insight is that convolution and pooling can cause underﬁtting.\n",
    "    - If a task relies on preserving precisionspatial information, then using pooling on all features can cause underﬁtting.\n",
    "* Another key insight from this view is that we should only compare convolutional models to other convolutional models in benchmarks of statistical learningperformance.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.5 Variants of the Basic Convolution Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, the input is usually not just a grid of real values. Rather, it is agrid of vector-valued observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-D tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For example, a color image has a <font color=\"red\">red, green,and blue</font> intensity at each pixel. \n",
    "* In a multilayer convolutional network, the inputto the second layer is the output of the ﬁrst layer, which usually has the outputof many diﬀerent convolutions at each position. \n",
    "* When working with images, we usually think of the <font color=\"red\">input and output of the convolution as being 3-D tensors</font>, with <font color=\"blue\">one index into the diﬀerent channels and two indices into the spatial coordinates</font> of each channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-D tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Assume we have a 4-D kernel tensor \n",
    "    - K with element K_i,j,k,l \n",
    "        - giving the connection strength \n",
    "        - between \n",
    "            - a unit in channel i of the output and \n",
    "            - a unit in channelj of the input, \n",
    "        - with an oﬀset of k rows and l columns \n",
    "            - between the output unit and the input unit. \n",
    "    - Assume our input consists of observed data V with elementV_i,j,k giving the value of the input unit within channel i at row j and column k.\n",
    "    - If Z is produced by convolving K across V without ﬂipping K, then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.18.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also want to skip over some positions of the kernel in order to reducethe computational cost (at the expense of not extracting our features as ﬁnely).\n",
    "\n",
    "* We can think of this as downsampling the output of the full convolution function.\n",
    "* If we want to sample only every s pixels in each direction in the output, then we can deﬁned a downsampled convolution function c such that :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.19.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We refer to s as the stride of this downsampled convolution.\n",
    "* It is also possibleto deﬁne a separate stride for each direction of motion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### zero-padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One essential feature of any convolutional network implementation is the ability to implicitly zero-pad the input V in order to make it wider.\n",
    "\n",
    "* Without thisfeature, the width of the representation shrinks by the kernel width - 1 at eac hlayer. \n",
    "* Zero padding the input allows us to control the kernel width and the size ofthe output independently. \n",
    "* Without zero padding, we are forced to choose between shrinking the spatial extent of the network rapidly and using small kernels–both scenarios that signiﬁcantly limit the expressive power of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.20.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.21.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Three special cases of the zero-padding setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* valid convolution\n",
    "    - One is the extreme case in which no zero-padding is used whatsoever, and the convolution kernel is only allowed to visit positions where the entire kernel is contained entirely within the image.\n",
    "    - In MATLAB terminology, this is called valid convolution.\n",
    "    - this case, all pixels in the output are a function of the same number of pixels inthe input, so the behavior of an output pixel is somewhat more regular. However,the size of the output shrinks at each layer.\n",
    "    - As layers are added, the spatial dimension of the network will eventually drop to 1 × 1, at which point additional layers cannot meaningfully be considered convolutional. \n",
    "* same convolution\n",
    "    - Another special case of the zero-padding settingis when just enough zero-padding is added to keep the size of the output equalto the size of the input. \n",
    "    - MATLAB calls this same convolution.\n",
    "    - In this case,the network can contain as many convolutional layers as the available hardware can support, since the operation of convolution does not modify the architectural possibilities available to the next layer.\n",
    "    - However, the input pixels near the borderinﬂuence fewer output pixels than the input pixels near the center. This canmake the border pixels somewhat underrepresented in the model.\n",
    "* full convolution\n",
    "    - MATLAB refers to as full convolution, in which enough zeroes are added for every pixel to be visited k times in each direction,resulting in an output image of size m+k −1×m + k −1. \n",
    "    * In this case, the output pixels near the border are a function of fewer pixels than the output pixels nearthe center. \n",
    "    * This can make it diﬃcult to learn a single kernel that performs wellat all positions in the convolutional feature map. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually the optimal amount ofzero padding (in terms of test set classiﬁcation accuracy) lies somewhere between“valid” and “same” convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In some cases, we do not actually want to use convolution, but rather locallyconnected layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this case, the adjacency matrix in the graph of our MLP isthe same, but every connection has its own weight, speciﬁed by a 6-D tensor W.\n",
    "* The indices into W are respectively: \n",
    "    - i, the output channel, \n",
    "    - j, the output row, \n",
    "    - k,the output column, \n",
    "    - l, the input channel, \n",
    "    - m, the row oﬀset within the input, and \n",
    "    - n, the column oﬀset within the input. \n",
    "* The linear part of a locally connected layeris then given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.22.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tiled convolution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiled convolution (Gregor and LeCun, 2010; Le et al., 2010) oﬀers a compro-mise between a convolutional layer and a locally connected layer. Rather thanlearning a separate set of weights at every spatial location, we learn a set of kernelsthat we rotate through as we move through space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that immediatelyneighboring locations will have diﬀerent ﬁlters, like in a locally connected layer,but the memory requirements for storing the parameters will increase only by afactor of the size of this set of kernels, rather than the size of the entire outputfeature map. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deﬁne tiled convolution algebraically, let k be a 6-D tensor, where two ofthe dimensions correspond to diﬀerent locations in the output map. Rather thanhaving a separate index for each location in the output map, output locationscycle through a set of t diﬀerent choices of kernel stack in each direction. If t isequal to the output width, this is the same as a locally connected layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.23.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the network, we need to compute the derivatives with respect to theweights in the kernel. To do so, we can use a function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.24.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this layer is not the bottom layer of the network, we’ll need to compute thegradient with respect to V in order to backpropagate the error farther down. Todo so, we can use a function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.25.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also use h to deﬁne the reconstruction of a convolutional autoencoder, or the probability distribution over visible given hidden units in a convo-lutional RBM or sparse coding model. Suppose we have hidden units H in thesame format as Z and we deﬁne a reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.26.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train the autoencoder, we will receive the gradient with respectto R as a tensor E. To train the decoder, we need to obtain the gradient withrespect to K. This is given by g(H, E, s). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the encoder, we need to obtainthe gradient with respect to H. This is given by c(K, E, s). It is also possible todiﬀerentiate through g using c and h, but these operations are not needed for thebackpropagation algorithm on any standard network architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.6 Structured Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.7 Convolutional Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.8 Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used with a convolutional network usually consists of several channels,each channel being the observation of a diﬀerent quantity at some point in spaceor time. See Table 9.1 for examples of data types with diﬀerent dimensionalitiesand number of channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have discussed only the case where every example in the trainand test data has the same spatial dimensions. One advantage to convolutionalnetworks is that they can also process inputs with varying spatial extents. Thesekinds of input simply cannot be represented by traditional, matrix multiplication-based neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.27.png\" width=600 />\n",
    "<img src=\"figures/cap9.28.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 9.1: Examples of diﬀerent formats of data that can be used with convolutional networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.9 Eﬃcient Convolution Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution is equivalent to converting both the input and the kernel to thefrequency domain using a Fourier transform, performing point-wise multiplicationof the two signals, and converting back to the time domain using an inverseFourier transform. For some problem sizes, this can be faster than the naiveimplementation of discrete convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### separable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When a d-dimensional kernel can be expressed as the outer product of dvectors, one vector per dimension, the kernel is called separable. When the kernelis separable, naive convolution is ineﬃcient. \n",
    "* It is equivalent to compose d one-dimensional convolutions with each of these vectors. The composed approachis signiﬁcantly faster than performing one k-dimensional convolution with theirouter product. \n",
    "* The kernel also takes fewer parameters to represent as vectors.\n",
    "* If the kernel is w elements wide in each dimension, then naive multidimensionalconvolution requires O(wd) runtime and parameter storage space, while separableconvolution requires O(w × d) runtime and parameter storage space. \n",
    "* Of course,not every convolution can be represented in this way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.10 Random or Unsupervised Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, the most expensive part of convolutional network training is learningthe features. The output layer is usually relatively inexpensive due to the smallnumber of features provided as input to this layer after passing through severallayers of pooling. When performing supervised training with gradient descent,every gradient step requires a complete run of forward propagation and backwardpropagation through the entire network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### unsupervised fashion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to reduce the cost of convo-lutional network training is to use features that are not trained in a supervised fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two basic strategies for obtaining convolution kernels without supervised training. \n",
    "* One is to simply initialize them randomly. \n",
    "* The other is tolearn them with an unsupervised criterion.\n",
    "    - This approach allows the features tobe determined separately from the classiﬁer layer at the top of the architecture.One can then extract the features for the entire training set just once, essentiallyconstructing a new training set for the last layer. Learning the last layer is thentypically a convex optimization problem, assuming the last layer is something likelogistic regression or an SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random ﬁlters often work surprisingly well in convolutional networks (Jar-rett et al., 2009b; Saxe et al., 2011; Pinto et al., 2011; Cox and Pinto, 2011).Saxe et al. (2011) showed that layers consisting of convolution following by pool-ing naturally become frequency selective and translation invariant when assignedrandom weights. They argue that this provides an inexpensive way to choose thearchitecture of a convolutional network: ﬁrst evaluate the performance of severalconvolutional network architectures by training only the last layer, then take thebest of these architectures and train the entire architecture using a more expensiveapproach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An intermediate approach is to learn the features, but using methods that do not require full forward and back-propagation at every gradient step. Aswith multilayer perceptrons, we use greedy layer-wise unsupervised pretraining,to train the ﬁrst layer in isolation, then extract all features from the ﬁrst layeronly once, then train the second layer in isolation given those features, and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with other approaches to unsupervised pretraining, it remains diﬃcult totease apart the cause of some of the beneﬁts seen with this approach. Unsuper-vised pretraining may oﬀer some regularization relative to supervised training,or it may simply allow us to train much larger architectures due to the reducedcomputational cost of the learning rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.11 The Neuroscientiﬁc Basis for Convolutional Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.29.png\" width=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.30.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.31.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.12 Convolutional Networks and the History of DeepLearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.32.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* [1] bengio's book - Chapter 9 Convolutional Networks - http://www.iro.umontreal.ca/~bengioy/dlbook/version-07-08-2015/convnets.html\n",
    "* [2] Linear Systems and Convolution - http://www.slideshare.net/lineking/lecture4-26782530\n",
    "* [3] Convolutional Neural Networks: architectures, convolution / pooling layers - http://vision.stanford.edu/teaching/cs231n/slides/lecture7.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
