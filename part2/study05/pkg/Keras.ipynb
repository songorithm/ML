{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [keras.io](http://keras.io/)\n",
    "- [github.com/fchollet/keras](https://github.com/fchollet/keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- keras는 Python Theano 기반의 deep learning 도구이다\n",
    "- 쉽고 빠르게 Prototype을 만들수 있다 \n",
    "- convolution network과 recurrent networks 그리고 합쳐진 형태도 지원한다 \n",
    "- 임의의 연결이 가능하다 (supports arbitrary connectivity schemes)\n",
    "- CPU와 GPU연산을 지원한다\n",
    "\n",
    "### 설치하기\n",
    "\n",
    "- 기본적으로 numpy, scipy, theano 가 필요하며\n",
    "- 선택적으로 HDF5, h5py, cuDNN, 등이 필요하다\n",
    "- 설치는 \n",
    "- sudo pip install keras\n",
    "- 하면 알아서 다 깔아 준다 \n",
    "- 또는 \n",
    "- git clone  https://github.com/fchollet/keras.git\n",
    "- cd keras  (download 받은 폴더로 간다)\n",
    "- sudo python setup.py install\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Activation\n",
    "\n",
    "model.add(Dense(output_dim=64, input_dim=100, init=\"glorot_uniform\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(output_dim=10, init=\"glorot_uniform\"))\n",
    "model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/home/zena/Python2.7/lib/python2.7/site-packages/keras/layers/core.pyc in get_output(self, train)\n",
    "    409     def get_output(self, train=False):\n",
    "    410         X = self.get_input(train)\n",
    "--> 411         return self.activation(X)\n",
    "    412 \n",
    "    413     def get_config(self):\n",
    "\n",
    "/home/zena/Python2.7/lib/python2.7/site-packages/keras/activations.pyc in relu(x)\n",
    "     18 \n",
    "     19 def relu(x):\n",
    "---> 20     return T.nnet.relu(x)\n",
    "     21 \n",
    "     22 \n",
    "\n",
    "AttributeError: 'module' object has no attribute 'relu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ sudo pip uninstall theano\n",
    "$ sudo pip install git+git://github.com/Theano/Theano.git\n",
    "\n",
    "Kernel reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9, nesterov=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.fit(X_train, Y_train, nb_epoch=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.train_on_batch(X_batch, Y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "objective_score = model.evaluate(X_test, Y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classes = model.predict_classes(X_test, batch_size=32)\n",
    "proba = model.predict_proba(X_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[imdb_cnn.py](https://github.com/fchollet/keras/blob/cb77f7d7e2933078838e836018fdfa7075e5385f/examples/imdb_cnn.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "20000 train sequences\n",
      "5000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (20000, 100)\n",
      "X_test shape: (5000, 100)\n",
      "Build model...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 824s - loss: 0.6401 - acc: 0.5930 - val_loss: 0.5144 - val_acc: 0.7442\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 823s - loss: 0.3908 - acc: 0.8255 - val_loss: 0.3615 - val_acc: 0.8344\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 817s - loss: 0.3173 - acc: 0.8636 - val_loss: 0.3788 - val_acc: 0.8256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7133c10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "'''\n",
    "    This example demonstrates the use of Convolution1D\n",
    "    for text classification.\n",
    "    Run on GPU: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python imdb_cnn.py\n",
    "    Get to 0.8330 test accuracy after 3 epochs. 100s/epoch on K520 GPU.\n",
    "'''\n",
    "\n",
    "# set parameters:\n",
    "max_features = 5000\n",
    "maxlen = 100\n",
    "batch_size = 32\n",
    "embedding_dims = 100\n",
    "nb_filter = 250\n",
    "filter_length = 3\n",
    "hidden_dims = 250\n",
    "nb_epoch = 3\n",
    "\n",
    "print(\"Loading data...\")\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features,\n",
    "                                                      test_split=0.2)\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# we add a Convolution1D, which will learn nb_filter\n",
    "# word group filters of size filter_length:\n",
    "model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                        filter_length=filter_length,\n",
    "                        border_mode=\"valid\",\n",
    "                        activation=\"relu\",\n",
    "                        subsample_length=1))\n",
    "# we use standard max pooling (halving the output of the previous layer):\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "\n",
    "# We flatten the output of the conv layer, so that we can add a vanilla dense layer:\n",
    "model.add(Flatten())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', class_mode=\"binary\")\n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, validation_data=(X_test, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
